{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b5932d-2702-4fc9-bb63-016dc5325236",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Questions:\n",
    "1. How to create Schema in PySpark?\n",
    "2. What are other ways to create it?\n",
    "3. What is StructField and StructType in Schema?\n",
    "4. What if I have header in my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cc520d4-2084-44cd-b142-506b9c892f47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can create Schema in Spark using any of the below processes:\n",
    "1. Using StructType and StructField (from pyspark.sql.types import StructType, StructField)\n",
    "  - StructType -> Defines the structure of the DF (List or Collection of StructField)\n",
    "  - StructField -> Structure of the Columns in the DF (name of the field, data type of the field, the field nullable or not)\n",
    "2. Using DDL (Data Definition Language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d341388e-7a71-4880-bd81-3bc2fd8a123c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a Schema using StructType and StructField:\\\n",
    "<pre>\n",
    "my_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"name\", StringType(), True),\n",
    "  StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "</pre>\n",
    "\n",
    "Create a Schema using DDL:\\\n",
    "<pre>\n",
    "ddl_my_schema = \"id integer, name string, age integer\"\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c030b5-73ff-4717-ad04-b16a052a0eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "my_schema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", IntegerType(), True)\n",
    "])\n",
    "flight_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"false\")\\\n",
    "            .option(\"skiprows\", 1)\\\n",
    "            .option(\"inferschema\", \"false\")\\\n",
    "            .schema(my_schema)\\\n",
    "            .option(\"mode\", \"FAILFAST\")\\\n",
    "            .load(\"/FileStore/tables/flight_data.csv\")\n",
    "# As header and inferschema both are false, spark will ignore the headers in the data set and will not infer the data in the cells while generating the DF\n",
    "# But as we have added our own custom schema, spark will consider this schema while generating the DF.\n",
    "# The headers in the DF will come from the Fields that we defined in StructField\n",
    "# We needed to make the the mode, PERMISSIVE, so the issue fields will simply have null in them. \n",
    "# We are using it as the 1st row has string in the count column. Which should have been integer.\n",
    "# That is because, the 1st row will show the pre-existing actual row in the dataset.\n",
    "# In this case, skiprows will skip the 1st row (as defined), the default header in the dataset \n",
    "# Once this is done, we can again change the mode to FAILFAST.\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3499db-4aa6-4035-ba84-9b970712fcb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Check if the uploaded file is present in the DB or not in Databricks\\\n",
    "<pre>\n",
    "%fs\n",
    "ls /FileStore/tables/flight_data.csv\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3074376275128034,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "creating_manual_schema",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
