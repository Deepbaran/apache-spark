{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb7ec2e-36e6-4162-9038-fa8cb415b1c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=8801661251094662#setting/sparkui/1107-041625-5ckgols2/driver-7133802880630204755\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8801661251094662#setting/sparkui/1107-041625-5ckgols2/driver-7133802880630204755\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5e352a-4521-49c8-a694-9f5019bc0070",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Core Structure:\\\n",
    "<pre>\n",
    "DataframeReader.format(...)\\\n",
    "            .option(\"key\", \"value\")\\\n",
    "            .schema(...)\\\n",
    "            .load(...)\n",
    "</pre>\n",
    "\n",
    "format [Optional] => Data file format. e.g. CSV, JSON, JDBC/ODBC, table, parquet (Default)\n",
    "option [Optional] => inferschema, mode, header\n",
    "schema [Optional] => manual schema can be passed\n",
    "load [Required] => Path where our data is residing\n",
    "\n",
    "Dataframe Reader API => To access this, use \"spark.read\"\n",
    "spark => SparkSession\n",
    "\n",
    "example:\\\n",
    "<pre>\n",
    "spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .option(\"mode\", \"FAILFAST\")\\\n",
    "    .load(\"c:\\user\\download\\data.csv\")\n",
    "</pre>\n",
    "\n",
    "Mode:\n",
    "1. FAILFAST -> Fail execution if malformed record is found in dataset\n",
    "2. DROPMALFORMED -> Drop the corrupted record and continue with the execution\n",
    "3. PERMISSIVE -> This is default. Set null value to all corrupted fields\n",
    "\n",
    "File formats:\n",
    "1. csv -> Comma Separated Value\n",
    "2. json -> javascript object notation\n",
    "3. parquet -> Column-oriented data file format designed for efficient data storage and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7efc05-9f74-4601-8676-8443a3a2c01e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|              _c0|                _c1|  _c2|\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"false\")\\\n",
    "            .option(\"inferschema\", \"false\")\\\n",
    "            .option(\"mode\", \"FAILFAST\")\\\n",
    "            .load(\"/FileStore/tables/flight_data-1.csv\")\n",
    "# Show 5 records | As header is false, so no header will be displayed\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c88e8eb-69a8-421e-9e53-9295cd2b85c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferschema\", \"false\")\\\n",
    "            .option(\"mode\", \"FAILFAST\")\\\n",
    "            .load(\"/FileStore/tables/flight_data-1.csv\")\n",
    "# Show 5 records\n",
    "# As header is true, header will be displayed\n",
    "flight_df_header.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfc2bc5-1c47-4e49-9509-7968ab41164e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: <bound method DataFrame.printSchema of DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: string]>"
     ]
    }
   ],
   "source": [
    "# Show schema\n",
    "# As inferschema is false, so schema is not inferred (meaning, do not try to read the data and try to figure out the data in it)\n",
    "# That's why count is coming as string rather than integer as it is not inferred.\n",
    "flight_df_header.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6171c04d-f9cd-4e00-9261-0f09eb60da2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "flight_df_header_schema = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferschema\", \"true\")\\\n",
    "            .option(\"mode\", \"FAILFAST\")\\\n",
    "            .load(\"/FileStore/tables/flight_data-1.csv\")\n",
    "# Show 5 records\n",
    "# As header is true, header will be displayed\n",
    "# Here inferredschema is true. So the schema will get inferred\n",
    "flight_df_header_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "501cd942-f83a-4063-9842-70042982c347",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: <bound method DataFrame.printSchema of DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]>"
     ]
    }
   ],
   "source": [
    "# Show schema\n",
    "# As inferschema is true, so schema is inferred (meaning, try to read the data and try to figure out the data in it)\n",
    "# That's why count is coming as int as the data in it is inferred and it was found that the data is type int and not string.\n",
    "flight_df_header_schema.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e142058-3a3e-4277-9baa-4c149fbe9e33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "read_csv_file_in_spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
