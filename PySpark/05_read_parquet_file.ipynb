{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c95b5e5-a0ca-4192-9bfa-1ff555123faf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Questions:\n",
    "1. What is parquet file format?\n",
    "2. Why do we need parquet?\n",
    "3. How to read parquet file?\n",
    "4. What makes parquet default choice?\n",
    "5. What encoding is done on date?\n",
    "6. What compression technique is used?\n",
    "7. How to optimize the parquet file?\n",
    "8. What is row group, column and pages?\n",
    "9. How projection pruning and predicate pushdown works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d512bc61-8f71-4dba-ae10-b6fcc41a0541",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Doc - [Link](https://parquet.apache.org/docs/concepts/)\n",
    "- Parquet is a file format.\n",
    "- Parquet is a Columnar based file format.\n",
    "\n",
    "\n",
    "Columnar Based File Format and Row Based File Format: [Link](https://blog.devgenius.io/big-data-file-formats-d980f5d07e44) | [Link](https://www.linkedin.com/pulse/day10-data-layout-row-based-vs-column-based-farhan-khan/)\n",
    "- In Big data, we do write once and read many.\n",
    "- Hence, using a file format which is faster to read is preferrable to use.\n",
    "- In case of Column oriented file formats, the way they are stored in disk, it becomes easier to read data from it.\n",
    "- That is beacuse, suppose we want to read column 1 and column 3 of a data. So, to do that, for row oriented data, we first read column1 of record1 and read it and then jump to column3 of record1 and read it and then jump to Column1 of record2 and read it and then jump to Column3 of record2 and read it and keep repeating this for the rest of the records, This is because, the data stored in row oriented fashion is not continuous and we need to make jumps in the memory. It takes more time. But in case of column oriented data, we first read the complete Column1 and then jump to where Column3 data are stored and read all the Column3 at once. This makes things easier as less memory jumps are required. Which is why reading column oriented data is way easier than row oriented data. But by this logic, row oriented data is faster to write than column oriented data.\n",
    "\n",
    "OLAP vs OLTP:\n",
    "- OLAP - Online Analytical Processing\n",
    "  - Only few columns are read.\n",
    "  - Column-Oriented file format is used. (Faster to read)\n",
    "- OLTP - Online Transactional Processing\n",
    "  - Write -> insert, update, delete\n",
    "  - Row-Oriented file format is used. (Faster to write)\n",
    "\n",
    "- At the end of the day we want to reduce cost and time and improve performance of our queries. For which we prefer to use column-oriented file format, Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b082fb9-57b5-40d5-be9f-c3f7da7b980e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<pre>\n",
    "File uploaded to /FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet\n",
    "</pre>\n",
    "gz is a compression technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0a68ca-7eac-4405-8113-cde96c51d6c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n|       United States|             Russia|  156|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet\")\n",
    "df.show()\n",
    "\n",
    "# We do not need to give extra parameters to read parquet files as it has many metadata in it to infer all of them itself.\n",
    "# Parquet is a binary file format, so we cannot normally read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e2f90ab-948e-44c7-b1ac-fffe7893bb31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Although Parquet is columnar based file format, but to read huge records of data faster, it divides the records into chunks/groups. So that reading becomes faster. So, we can say that in a way, Parquet uses both columnar and row based storage to make reading faster.\n",
    "- These chunk of records are called Row Group.\n",
    "  - Row Group is a logical partition to divide our data into smaller chunks for better read.\n",
    "  - The default size of a Row Group is 128 MB. \n",
    "  - Each Row Group contains its own Metadata (Data of data. Like who saved it, when was it saved, number of records, minimum size, max size, etc.).\n",
    "  - Each Row Group contains multiple Columns.\n",
    "  - Each Columns contain multible Pages.\n",
    "  - Each Page has metadata and Values.\n",
    "  - Values is our actual data.\n",
    "  - All of these meta data makes Parquet so good to read.\n",
    "- Parquet is a structured file format.\n",
    "- It is stored in Binary form.\n",
    "- [Link](https://karthiksharma1227.medium.com/understanding-parquet-and-its-optimization-opportunities-c7265a360391) | [Link](https://data-mozart.com/parquet-file-format-everything-you-need-to-know/) | [Link](https://towardsdatascience.com/demystifying-the-parquet-file-format-13adb0206705)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f86543-d3a9-4805-9bbc-edc413a4b4f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Commands to see and inspect metadata in local.\n",
    "<pre>\n",
    "pip install parquet-tools\n",
    "parquet-tools show (file_path)\n",
    "parquet-tools inspect (file_path)\n",
    "</pre>\n",
    "- Python commands to read more metada:\n",
    "<pre>\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "parquet_file = pq.ParquetFile(r'(file_path)')\n",
    "parquet_file.metadata\n",
    "parquet_file.metadata.row_group(0) \n",
    "parquet_file.metadata.row_group(0).column(0)\n",
    "parquet_file.metadata.row_group(0).column(0).statistics \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0dadb5e-e180-49b1-9314-c7dd1471db14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Parquet uses the below compression methods to compress the data more:\n",
    "  - First Parquet does Dictionary Compression\n",
    "  - Then it daoes RLE (Run Length Encoding) and bit-packing\n",
    "- Encoding Types:\n",
    "  - Gzip\n",
    "  - Snappy\n",
    "  - LZO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e908f3a8-f316-4069-bde5-e236fb229629",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Data Organization in Parquet:\n",
    "  - File\n",
    "    - Row Group (We have metadata at group level too)\n",
    "      - Column\n",
    "        - Pages\n",
    "          - Metadata\n",
    "            - min\n",
    "            - max\n",
    "            - count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2ff0230-7a7c-45da-8492-0490d3099569",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Optimization\n",
    "<pre>\n",
    "select * from table where age < 18\n",
    "</pre>\n",
    "- Now if we read this data from Parquet file, then we can quickly determine from which Row Groups we need to read the data (As each Row Group will have the min age and max age metadata) without going through each individual record. This makes the reading faster. Makes less IO operations, which saves cost, time and saves CPU utilization (Improved performance).\n",
    "\n",
    "Projection Pruning:\n",
    "- This means that, we should not select/read/scan the columns that are not rquired from the very beginning.\n",
    "- As we are avoiding unnecessary columns, parquet's hybrid model helps here and makes reading faster.\n",
    "\n",
    "Predicate Pushdown:\n",
    "- Predicate Pushdown prevent the Row Groups from getting scanned, whose metadata does not match with our query.\n",
    "- For the above example the Row groups with min age more than or equal to 18 will be discarded without getting scanned.\n",
    "\n",
    "Now for another example, consider the below query:\n",
    "<pre>\n",
    "select * from table where age = 18\n",
    "</pre>\n",
    "- Now, we know that Parquet does Dictionary Encoding of each of it's columns and store them as meta data in Row Group.\n",
    "- So, while going through, Row Groups, each Row Group that does not have dictionary key 18 for the age column in them, will be directly discarded.\n",
    "- This maked reading the data faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e181ac77-e00b-41dd-ae5a-c21899d395f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "read_parquet_file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
